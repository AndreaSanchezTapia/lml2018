{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEARNING MACHINE LEARNING 2018\n",
    "\n",
    "# Lab 2: Intro to Scikit-Learn: Linear and KNN Regression\n",
    "\n",
    "**Universidad del Rosario**<br>\n",
    "**July 23, 2018**<br>\n",
    "**Instructors: Pavlos Protopapas, Rafael Martinez, Juan Fernando Perez, Dora Suarez, Juan Felipe Giraldo, Dora Suarez, Julian Rincon, German Obando**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Scikit-learn\n",
    "\n",
    "### Installation\n",
    "Scikit-learn is included with the [Anaconda distribution of Pyhton](https://www.anaconda.com/download/). Current stable version is 0.19.2. \n",
    "\n",
    "You should therefore have it in  your systems. If you want to install it separetely in your own machine, you can find the installation and all other details in their [webpage](http://scikit-learn.org/stable/index.html). \n",
    "\n",
    "Scikit-learn requires:\n",
    "\n",
    "* Python (>= 2.7 or >= 3.3),\n",
    "* NumPy (>= 1.8.2),\n",
    "* SciPy (>= 0.13.3).\n",
    "\n",
    "Installation is easily performed with the `pip` or `conda` commands:\n",
    "\n",
    "`pip install -U scikit-learn`\n",
    "\n",
    "or\n",
    "\n",
    "`conda install scikit-learn`\n",
    "\n",
    "\n",
    "### Basic functionality\n",
    "\n",
    "`Scikit-learn` is the main `python` machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as `train_test_split`. It can be used in `python` by the incantation `import sklearn`.\n",
    "\n",
    "The library has a very well-defined interface. This makes the library a joy to use, and surely contributes to its popularity.  The API consists of three interfaces:\n",
    "1. estimator interface: builds and fits models\n",
    "   * This interface is at the core of the library\n",
    "   * Provides the `fit()` method\n",
    "   * Unsupervised and supervised learning algorithms can be accessed from the estimator interface\n",
    "2. predictor interface: makes predictions\n",
    "3. transformer interface: converts data\n",
    "\n",
    "You can refer to the original `scikit-learn` API paper for more details: [Buitinck, Lars, et al. \"API design for machine learning software: experiences from the scikit-learn project.\" arXiv preprint arXiv:1309.0238 (2013).](https://arxiv.org/abs/1309.0238)\n",
    "\n",
    "Let's see the structure of `scikit-learn` needed to make these fits. `.fit` always takes two arguments:\n",
    "```python\n",
    "  estimator.fit(Xtrain, ytrain)\n",
    "```\n",
    "We will consider two estimators in this lab: `LinearRegression` and `KNeighborsRegressor`.\n",
    "\n",
    "Critically, `Xtrain` must be in the form of an *array of arrays*, with the inner arrays each corresponding to one sample, and whose elements correspond to the feature values for that sample (visuals coming in a moment).\n",
    "\n",
    "`ytrain` on the other hand is a simple array of responses.  These are continuous for regression problems.\n",
    "\n",
    "![](scikitlearn1.jpg)\n",
    "\n",
    "\n",
    "So it is very easy to use: Suppose you wanted to fit the following dataset with a particular regression estimator, for example the Linear Regression:\n",
    "\n",
    "![](scikitlearn2.png)\n",
    "\n",
    "Above, the predictor $x$ is along the $x$-axis, whereus the dependent variable $y$ is along the $y$-axis. The goal is simple: use the dataset to learn the model and then predict for new, unseen values of $x$. With the structure outlined above, this is very easy to code in sklearn:\n",
    "\n",
    "![](scikitlearn3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with sklearn\n",
    "\n",
    "### Table of Contents \n",
    "\n",
    "<ol>\n",
    "<li> Linear regression with a toy  </li>\n",
    "- matrices and math <br>\n",
    "- building a model from scratch<br>\n",
    "- building a model with statsmodel and sklearn\n",
    "<li> Simple linear regression with automobile data </li>\n",
    "<li> Multiple linear regression with automobile data </li>\n",
    "<li> Interpreting results</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Linear regression with a toy \n",
    "We first examine a toy problem, focusing our efforts on fitting a linear model to a small dataset with three observations.  Each observation consists of one predictor $x_i$ and one response $y_i$ for $i = 1, 2, 3$,\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "To be very concrete, let's set the values of the predictors and responses.\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(1, 2), (2, 2), (3, 4)\\}\n",
    "\\end{equation*}\n",
    "\n",
    "There is no line of the form $\\beta_0 + \\beta_1 x = y$ that passes through all three observations, since the data is not collinear.  Thus our aim is to find the line that best fits these observations in the *least-squares sense*, as discussed in lecture.\n",
    "\n",
    "### Matrices and math \n",
    "\n",
    "Suspending reality, suppose there is a line $\\beta_0 + \\beta_1 x = y$ that passes through all three observations.  Then we'd solve\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_0 + \\beta_1 &=& 2 \\nonumber \\\\\n",
    "\\beta_0 + 2 \\beta_1 &=& 2 \\nonumber \\\\\n",
    "\\beta_0 + 3 \\beta_1 &=& 4, \\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "for  $\\beta_0$ and  $\\beta_1$, the intercept and slope of the desired line.  Let's write these equations in matrix form.  The left hand sides of the above equations can be written as\n",
    "\n",
    "![](LHS.jpg)\n",
    "\n",
    "\n",
    "while the right hand side is simply the vector\n",
    "\n",
    "\\begin{equation*}Y = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "4 \n",
    "\\end{bmatrix}. \\end{equation*}\n",
    "\n",
    "Thus we have the matrix equation $X \\beta = Y$ where\n",
    "\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 2\\\\\n",
    "1 & 3\n",
    "\\end{bmatrix}, \\quad\n",
    "\\beta = \\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \n",
    "\\end{pmatrix}, \\quad \\mathrm{and} \n",
    "\\quad Y = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "4 \n",
    "\\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "To find the best possible solution to this linear system that has no solution, we need to solve the *normal equations*, or\n",
    "\n",
    "\\begin{equation}\n",
    "X^T X \\beta = X^T Y.\n",
    "\\end{equation}\n",
    "\n",
    "If $X^T X$ is invertible then the solution is\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (X^T X)^{-1} X^T Y.\n",
    "\\end{equation}\n",
    "\n",
    "The above solution comes from minimizing the least-square error for this system of equation that does not have an exact solution. We can expand this solution to show that in fact, the values $\\hat{\\beta}$ that best approximate the (non-existent) solution of the system above is are given by this lat equation ([Here](http://mlwiki.org/index.php/Normal_Equation) is a link to understand this, for thos fond of linear algebra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** What if the toy problem included a second predictor variable?  How would $X, \\beta$, and $Y$ change, if at all?  Would anything else change?  Create a new markdown cell below and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your text here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model from scratch\n",
    "\n",
    "We now solve the normal equations to find the best fit solution to our toy problem.   Note that we have constructed our toy problem so that $X^T X$ is invertible.  Let's import the needed modules.  Note that we've imported statsmodels and sklearn in this below, which we'll use to build regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmartine/anaconda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippets of code below solves the equations using the observed predictors and responses, which we'll call the training data set.  Let's walk through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "#observed predictors\n",
    "x_train = np.array([1, 2, 3])\n",
    "# or do this, which creates 3 x 1 vector so no need to reshape\n",
    "#x_train = np.array([[1], [2], [3]])   \n",
    "print(x_train.shape)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(len(x_train),1)\n",
    "#check dimensions \n",
    "print(x_train.shape)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[2]\n",
      " [2]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "#observed responses\n",
    "y_train = np.array([2, 2, 4])\n",
    "# or do this, which creates 3 x 1 vector so no need to reshape\n",
    "#y_train = np.array([[2], [2], [4]])\n",
    "y_train = y_train.reshape(len(y_train),1)\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 2.]\n",
      " [1. 3.]] (3, 2)\n"
     ]
    }
   ],
   "source": [
    "#build matrix X by concatenating predictors and a column of ones\n",
    "n = x_train.shape[0]\n",
    "ones_col = np.ones((n, 1))\n",
    "X = np.concatenate((ones_col, x_train), axis=1)\n",
    "#check X and dimensions\n",
    "print(X, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  6.]\n",
      " [ 6. 14.]]\n",
      "[[ 8.]\n",
      " [18.]]\n"
     ]
    }
   ],
   "source": [
    "#matrix X^T X\n",
    "LHS = np.dot(np.transpose(X), X)\n",
    "print(LHS)\n",
    "\n",
    "#matrix X^T Y\n",
    "RHS = np.dot(np.transpose(X), y_train)\n",
    "print(RHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#solution beta to normal equations, since LHS is invertible by toy construction\n",
    "betas = np.dot(np.linalg.inv(LHS), RHS)\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667] [1.]\n"
     ]
    }
   ],
   "source": [
    "#intercept beta0\n",
    "beta0 = betas[0]\n",
    "\n",
    "#slope beta1\n",
    "beta1 = betas[1]\n",
    "\n",
    "print(beta0, beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Turn the code from the above cells into a function, called `simple_linear_regression_fit`, that inputs the training data and returns `beta0` and `beta1`.\n",
    "\n",
    "> To do this, copy and paste the code from the above cells below and adjust the code as needed, so that the training data becomes the input and the betas become the output.\n",
    "\n",
    "> Check your function by calling it with the training data from above and printing out the beta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "def simple_linear_regression_fit(x_train, y_train):\n",
    "    \n",
    "    \n",
    "    \n",
    "    return betas\n",
    "\n",
    "\n",
    "print(\"(beta0, beta1) = (%f, %f)\" %(beta0, beta1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "#beta 1 > 0 which is reasonable given the data.  the best fit line should have a positive slope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model with statsmodel and sklearn\n",
    "\n",
    "Now that we can concretely fit the training data from scratch, let's learn two Python packages to do it all for us: [statsmodels](http://www.statsmodels.org/stable/regression.html) and [scikit-learn (sklearn)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).  Our goal  is to show how to implement simple linear regression with these packages.  For an important sanity check, we compare the $\\beta$ values from statsmodel and sklearn to the $\\beta$ values that we found from above from scratch.\n",
    "\n",
    "For the purposes of this lab, statsmodels and sklearn do the same thing.  More generally though, statsmodels tends to be easier for inference, whereas sklearn has machine-learning algorithms and is better for prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for statsmodels.  Statsmodels does not by default include the column of ones in the $X$ matrix, so we include it with `sm.add_constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 2.]\n",
      " [1. 3.]]\n",
      "(beta0, beta1) = (0.666667, 1.000000)\n"
     ]
    }
   ],
   "source": [
    "#create the X matrix by appending a column of ones to x_train\n",
    "X = sm.add_constant(x_train)\n",
    "#this is the same matrix as in our scratch problem!\n",
    "print(X)\n",
    "#build the OLS model (ordinary least squares) from the training data\n",
    "toyregr_sm = sm.OLS(y_train, X)\n",
    "#save regression info (parameters, etc) in results_sm\n",
    "results_sm = toyregr_sm.fit()\n",
    "#pull the beta parameters out from results_sm\n",
    "beta0_sm = results_sm.params[0]\n",
    "beta1_sm = results_sm.params[1]\n",
    "\n",
    "print(\"(beta0, beta1) = (%f, %f)\" %(beta0_sm, beta1_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the beta parameters, `results_sm` contains a ton of other potentially useful information.  Type `results_sm` and hit tab to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(beta0, beta1) = (0.666667, 1.000000)\n"
     ]
    }
   ],
   "source": [
    "#build the least squares model\n",
    "toyregr_skl = linear_model.LinearRegression()\n",
    "#save regression info (parameters, etc) in results_skl\n",
    "results_skl = toyregr_skl.fit(x_train,y_train)\n",
    "#pull the beta parameters out from results_skl\n",
    "beta0_skl = results_skl.intercept_\n",
    "beta1_skl = results_skl.coef_[0]\n",
    "\n",
    "print(\"(beta0, beta1) = (%f, %f)\" %(beta0_skl, beta1_skl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should feel pretty good about ourselves now, and we're ready to move on to a real problem!\n",
    "\n",
    "### Polynomial regression (inspired by J. Vanderplas blog)\n",
    "\n",
    "But before, let us briefly consider polynomial regression, which is a special case of linear regression. This should be good to realize that \"linear\" does not always mean straight lines. As long as you are linear on the predictors (even if they have a polynomial nature), the model is still linear.\n",
    "\n",
    "Polynomial regression is a special case of linear regression. With the main idea of how do you select your features. Looking at the multivariate regression with 2 variables: $x_1$ and $x_2$. Linear regression will look like this: $y = a_1x_1 + a_2x_2$.\n",
    "\n",
    "Now you want to have a polynomial regression (let's make 2 degree polynomial). We will create a few additional features: $x_1x_2$, $x_1^2$ and $x_2^2$. Our linear regression now looks like this: $y = a_1x_1 + a_2x_2 + a_3x_1x_2 + a_4x_1^2 + a_5x_2^2$\n",
    "\n",
    "This polynomial regression is easily implemente on sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([4, 2, 1, 3, 7])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can be stubborn and fit a line to the data using LinearRegression and get the optimal result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = x[:, np.newaxis]\n",
    "model = LinearRegression().fit(X, y)\n",
    "yfit = model.predict(X)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that we need a more sophisticated model to describe the relationship between $x$ and y.\n",
    "\n",
    "One approach to this is to transform the data, adding extra columns of features to drive more flexibility in the model. For example, we can add polynomial features to the data this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derived feature matrix has one column representing $x$, and a second column representing $x^2$, and a third column representing $x^3$. Computing a linear regression on this expanded input gives a much closer fit to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X2, y)\n",
    "yfit = model.predict(X2)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea of improving a model not by changing the model, but by transforming the inputs, is fundamental to many of the more powerful machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Simple linear regression with automobile data\n",
    "We will now use sklearn to predict automobile milesage per gallon (mpg) and evaluate these predictions. We first load the data and split them into a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Valiant</td>\n",
       "      <td>18.1</td>\n",
       "      <td>6</td>\n",
       "      <td>225.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.460</td>\n",
       "      <td>20.22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Duster 360</td>\n",
       "      <td>14.3</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>245</td>\n",
       "      <td>3.21</td>\n",
       "      <td>3.570</td>\n",
       "      <td>15.84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Merc 240D</td>\n",
       "      <td>24.4</td>\n",
       "      <td>4</td>\n",
       "      <td>146.7</td>\n",
       "      <td>62</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.190</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Merc 230</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>140.8</td>\n",
       "      <td>95</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.150</td>\n",
       "      <td>22.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Merc 280</td>\n",
       "      <td>19.2</td>\n",
       "      <td>6</td>\n",
       "      <td>167.6</td>\n",
       "      <td>123</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.440</td>\n",
       "      <td>18.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Merc 280C</td>\n",
       "      <td>17.8</td>\n",
       "      <td>6</td>\n",
       "      <td>167.6</td>\n",
       "      <td>123</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.440</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Merc 450SE</td>\n",
       "      <td>16.4</td>\n",
       "      <td>8</td>\n",
       "      <td>275.8</td>\n",
       "      <td>180</td>\n",
       "      <td>3.07</td>\n",
       "      <td>4.070</td>\n",
       "      <td>17.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Merc 450SL</td>\n",
       "      <td>17.3</td>\n",
       "      <td>8</td>\n",
       "      <td>275.8</td>\n",
       "      <td>180</td>\n",
       "      <td>3.07</td>\n",
       "      <td>3.730</td>\n",
       "      <td>17.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Merc 450SLC</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8</td>\n",
       "      <td>275.8</td>\n",
       "      <td>180</td>\n",
       "      <td>3.07</td>\n",
       "      <td>3.780</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cadillac Fleetwood</td>\n",
       "      <td>10.4</td>\n",
       "      <td>8</td>\n",
       "      <td>472.0</td>\n",
       "      <td>205</td>\n",
       "      <td>2.93</td>\n",
       "      <td>5.250</td>\n",
       "      <td>17.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Lincoln Continental</td>\n",
       "      <td>10.4</td>\n",
       "      <td>8</td>\n",
       "      <td>460.0</td>\n",
       "      <td>215</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.424</td>\n",
       "      <td>17.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chrysler Imperial</td>\n",
       "      <td>14.7</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>230</td>\n",
       "      <td>3.23</td>\n",
       "      <td>5.345</td>\n",
       "      <td>17.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fiat 128</td>\n",
       "      <td>32.4</td>\n",
       "      <td>4</td>\n",
       "      <td>78.7</td>\n",
       "      <td>66</td>\n",
       "      <td>4.08</td>\n",
       "      <td>2.200</td>\n",
       "      <td>19.47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Honda Civic</td>\n",
       "      <td>30.4</td>\n",
       "      <td>4</td>\n",
       "      <td>75.7</td>\n",
       "      <td>52</td>\n",
       "      <td>4.93</td>\n",
       "      <td>1.615</td>\n",
       "      <td>18.52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Toyota Corolla</td>\n",
       "      <td>33.9</td>\n",
       "      <td>4</td>\n",
       "      <td>71.1</td>\n",
       "      <td>65</td>\n",
       "      <td>4.22</td>\n",
       "      <td>1.835</td>\n",
       "      <td>19.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Toyota Corona</td>\n",
       "      <td>21.5</td>\n",
       "      <td>4</td>\n",
       "      <td>120.1</td>\n",
       "      <td>97</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.465</td>\n",
       "      <td>20.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Dodge Challenger</td>\n",
       "      <td>15.5</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.520</td>\n",
       "      <td>16.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AMC Javelin</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.435</td>\n",
       "      <td>17.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Camaro Z28</td>\n",
       "      <td>13.3</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>245</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.840</td>\n",
       "      <td>15.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Pontiac Firebird</td>\n",
       "      <td>19.2</td>\n",
       "      <td>8</td>\n",
       "      <td>400.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.845</td>\n",
       "      <td>17.05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Fiat X1-9</td>\n",
       "      <td>27.3</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>66</td>\n",
       "      <td>4.08</td>\n",
       "      <td>1.935</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Porsche 914-2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.3</td>\n",
       "      <td>91</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.140</td>\n",
       "      <td>16.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Lotus Europa</td>\n",
       "      <td>30.4</td>\n",
       "      <td>4</td>\n",
       "      <td>95.1</td>\n",
       "      <td>113</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.513</td>\n",
       "      <td>16.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ford Pantera L</td>\n",
       "      <td>15.8</td>\n",
       "      <td>8</td>\n",
       "      <td>351.0</td>\n",
       "      <td>264</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.170</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Ferrari Dino</td>\n",
       "      <td>19.7</td>\n",
       "      <td>6</td>\n",
       "      <td>145.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.770</td>\n",
       "      <td>15.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Maserati Bora</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>301.0</td>\n",
       "      <td>335</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.570</td>\n",
       "      <td>14.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Volvo 142E</td>\n",
       "      <td>21.4</td>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>109</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.780</td>\n",
       "      <td>18.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  \\\n",
       "0             Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1   \n",
       "1         Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1   \n",
       "2            Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1   \n",
       "3        Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0   \n",
       "4     Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0   \n",
       "5               Valiant  18.1    6  225.0  105  2.76  3.460  20.22   1   0   \n",
       "6            Duster 360  14.3    8  360.0  245  3.21  3.570  15.84   0   0   \n",
       "7             Merc 240D  24.4    4  146.7   62  3.69  3.190  20.00   1   0   \n",
       "8              Merc 230  22.8    4  140.8   95  3.92  3.150  22.90   1   0   \n",
       "9              Merc 280  19.2    6  167.6  123  3.92  3.440  18.30   1   0   \n",
       "10            Merc 280C  17.8    6  167.6  123  3.92  3.440  18.90   1   0   \n",
       "11           Merc 450SE  16.4    8  275.8  180  3.07  4.070  17.40   0   0   \n",
       "12           Merc 450SL  17.3    8  275.8  180  3.07  3.730  17.60   0   0   \n",
       "13          Merc 450SLC  15.2    8  275.8  180  3.07  3.780  18.00   0   0   \n",
       "14   Cadillac Fleetwood  10.4    8  472.0  205  2.93  5.250  17.98   0   0   \n",
       "15  Lincoln Continental  10.4    8  460.0  215  3.00  5.424  17.82   0   0   \n",
       "16    Chrysler Imperial  14.7    8  440.0  230  3.23  5.345  17.42   0   0   \n",
       "17             Fiat 128  32.4    4   78.7   66  4.08  2.200  19.47   1   1   \n",
       "18          Honda Civic  30.4    4   75.7   52  4.93  1.615  18.52   1   1   \n",
       "19       Toyota Corolla  33.9    4   71.1   65  4.22  1.835  19.90   1   1   \n",
       "20        Toyota Corona  21.5    4  120.1   97  3.70  2.465  20.01   1   0   \n",
       "21     Dodge Challenger  15.5    8  318.0  150  2.76  3.520  16.87   0   0   \n",
       "22          AMC Javelin  15.2    8  304.0  150  3.15  3.435  17.30   0   0   \n",
       "23           Camaro Z28  13.3    8  350.0  245  3.73  3.840  15.41   0   0   \n",
       "24     Pontiac Firebird  19.2    8  400.0  175  3.08  3.845  17.05   0   0   \n",
       "25            Fiat X1-9  27.3    4   79.0   66  4.08  1.935  18.90   1   1   \n",
       "26        Porsche 914-2  26.0    4  120.3   91  4.43  2.140  16.70   0   1   \n",
       "27         Lotus Europa  30.4    4   95.1  113  3.77  1.513  16.90   1   1   \n",
       "28       Ford Pantera L  15.8    8  351.0  264  4.22  3.170  14.50   0   1   \n",
       "29         Ferrari Dino  19.7    6  145.0  175  3.62  2.770  15.50   0   1   \n",
       "30        Maserati Bora  15.0    8  301.0  335  3.54  3.570  14.60   0   1   \n",
       "31           Volvo 142E  21.4    4  121.0  109  4.11  2.780  18.60   1   1   \n",
       "\n",
       "    gear  carb  \n",
       "0      4     4  \n",
       "1      4     4  \n",
       "2      4     1  \n",
       "3      3     1  \n",
       "4      3     2  \n",
       "5      3     1  \n",
       "6      3     4  \n",
       "7      4     2  \n",
       "8      4     2  \n",
       "9      4     4  \n",
       "10     4     4  \n",
       "11     3     3  \n",
       "12     3     3  \n",
       "13     3     3  \n",
       "14     3     4  \n",
       "15     3     4  \n",
       "16     3     4  \n",
       "17     4     1  \n",
       "18     4     2  \n",
       "19     4     1  \n",
       "20     3     1  \n",
       "21     3     2  \n",
       "22     3     2  \n",
       "23     3     4  \n",
       "24     3     2  \n",
       "25     4     1  \n",
       "26     5     2  \n",
       "27     5     2  \n",
       "28     5     4  \n",
       "29     5     6  \n",
       "30     5     8  \n",
       "31     4     2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load mtcars\n",
    "dfcars=pd.read_csv(\"/Users/jmartine/teaching/LML2018/LML_Colombia/Labs/Resources/data/mtcars.csv\")\n",
    "dfcars=dfcars.rename(columns={\"Unnamed: 0\":\"name\"})\n",
    "dfcars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training set and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "#set random_state to get the same split every time\n",
    "traindf, testdf = train_test_split(dfcars, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 12), (25, 12), (7, 12))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing set is ~20% of the total data; training set is ~80%\n",
    "dfcars.shape, traindf.shape, testdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose the variables that we think will be good predictors for the dependent variable `mpg`. \n",
    "\n",
    ">**EXERCISE:**  Pick one variable to use as a predictor for simple linear regression.  Create a markdown cell below and discuss your reasons.  You may want to justify this with some visualizations.  Is there a second variable you'd like to use as well, say for multiple linear regression with two predictors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** With either sklearn or statsmodels, fit the training data using simple linear regression.  Use the model to make mpg predictions on testing set.  \n",
    "\n",
    "> Plot the data and the prediction.  \n",
    "\n",
    ">Print out the mean squared error for the training set and the testing set and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "#define  predictor and response for training set\n",
    "\n",
    "\n",
    "# define predictor and response for testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data first, and separate it into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "# create linear regression object with sklearn\n",
    "\n",
    "\n",
    "#your code here\n",
    "# train the model and make predictions\n",
    "\n",
    "\n",
    "\n",
    "#your code here\n",
    "#print out coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs for test set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs for training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now estimate the mean square errors for the training and test set to see how well we fit the data with our model in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MSE2= np.mean((y_train - regr.predict(x_train))**2)\n",
    "test_MSE2= np.mean((y_test - regr.predict(x_test))**2)\n",
    "print(\"The training MSE is %2f, the testing MSE is %2f\" %(train_MSE2, test_MSE2))\n",
    "\n",
    "# or with sklearn.metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_train, regr.predict(x_train)))\n",
    "print(mean_squared_error(y_test, regr.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the training MSE is lower than the testing MSE.  This makes sense since we expect the model to be a better fit for the training data (since we trained with it!) than the testing data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Multiple linear regression with automobile data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** With either sklearn or statsmodels, fit the training data using multiple linear regression with two predictors.  Use the model to make mpg predictions on testing set.  Print out the mean squared error for the training set and the testing set and compare.  \n",
    "\n",
    ">How do these training and testing mean squared errors compare to those from the simple linear regression?\n",
    "\n",
    ">Time permitting, repeat the training and testing with three predictors and calculate the mean squared errors.  How do these compare to the errors from the one and two predictor models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "\n",
    "#create linear regression object with sklearn\n",
    "\n",
    "\n",
    "#train the model \n",
    "\n",
    "\n",
    "#make predictions using the testing set\n",
    "\n",
    "\n",
    "#coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with three predictors\n",
    "\n",
    "\n",
    "#create linear regression object with sklearn\n",
    "\n",
    "\n",
    "#train the model \n",
    "\n",
    "\n",
    "#make predictions using the testing set\n",
    "\n",
    "\n",
    "#coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that both the training MSE and testing MSE are lower than the corresponding training and testing MSEs when we had one predictor.  So it seems that we'd prefer our model with two predictors than one predictor.  \n",
    "\n",
    "But we observe that with the two predictor model the training MSE is larger than the testing MSE.  It is possible for the training MSE to be higher than the testing MSE.  This will be discussed more in lecture.  One possibility is that the test set was too small, so that the model by chance fit it better than the training set.  Can you think of other reasons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4:  $k$-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we did a simple linear regression on the car data.\n",
    "\n",
    "Because we're good data scientists, we decide that a $k$ nearest-neighbor regression should also be performed.\n",
    "\n",
    "Now that you're familiar with the `sklearn` API, you're ready to do a KNN regression.  We can go a little quicker here.  Let's use $5$ nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knnreg = KNeighborsRegressor(n_neighbors=5)\n",
    "knnreg.fit(x_train, y_train)\n",
    "r2 = knnreg.score(x_test, y_test)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The $R^2$ error*\n",
    "\n",
    "This is a good moment to define the $R^2$ score. This score is the proportion of the variance in the dependent variable that is predictable from the independent variable. Intuitively, it gives a measurement of how far the measured values $y_i$ are from the predicted values $f(x_i)$. \n",
    "\n",
    "If $\\bar{y}$ is the mean of the $n$ observed values:\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i\n",
    "$$\n",
    "\n",
    "then the variability of the data is given by the three sum of squares formulas:\n",
    "\n",
    "\n",
    "* The total sum of squares:\n",
    "$$\n",
    "SS_{\\rm{tot}} = \\sum_i(y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "* The regression sum of squares:\n",
    "$$\n",
    "SS_{\\rm{reg}} = \\sum_i(f_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "* The residual sum of squares:\n",
    "$$\n",
    "SS_{\\rm{res}} = \\sum_i(y_i - f_i)^2\n",
    "$$\n",
    "\n",
    "The $R^2$ score is generally defined as:\n",
    "\n",
    "$$\n",
    "R^2 \\equiv 1 - \\frac{\\color{blue}{SS_{\\rm{res}}}}{\\color{red}{SS_{\\rm{tot}}}}\n",
    "$$\n",
    "\n",
    "This is better understood if we look at a figure (credit Wikipedia). The better the linear regression (on the right) fits the data in comparison to the simple average (on the left graph), the closer the value of $R^2$ is to 1:\n",
    "\n",
    "![](rsquared.png)\n",
    "\n",
    "\n",
    "*Distance functions*\n",
    "\n",
    "The knn algorithm predicts the output value of an unseen point $x_0$ by averaging the output values of the $k$ nearest points in the training set. In order to measure a \"distance\", a proper metric should be decided on. Some popular distance functions are:\n",
    "\n",
    "* The euclidean distance\n",
    "\n",
    "$$\n",
    "D = \\sqrt{ \\sum_{i=1}^k(x_i-y_i)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "* The Manhattan distance\n",
    "\n",
    "$$\n",
    "D = \\sum_{i=1}^k|x_i-y_i|\n",
    "$$\n",
    "\n",
    "* The Minkowski distance\n",
    "\n",
    "$$\n",
    "D = \\left( \\sum_{i=1}^k(|x_i-y_i|)^q \\right)^{1/q}\n",
    "$$\n",
    "\n",
    "In sklearn you can choose to use between these distances by changing the `p` parameter of the `KNeighborsRegressor` function.\n",
    "\n",
    "You can also decide to give different weights to the different $k$ neighbors considered in a given prediction, which might be desirable in certain cases. Using the `weights` parameter in `KNeighborsRegressor` you can choose between different weighting strategies. For example, you might want to give more weight to those points that are closest to your prediction points. The parameter can take values `uniform` or `distance`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "What is the $R^{2}$ score on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnreg.fit(x_train, y_train)\n",
    "r2 = knnreg.score(x_train, y_train)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets vary the number of neighbors and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regdict = {}\n",
    "# Do a bunch of KNN regressions\n",
    "for k in [1, 2, 4, 6, 8, 10, 15]:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors=k)\n",
    "    knnreg.fit(x_train, y_train)\n",
    "    print(knnreg.score(x_train, y_train)) # print the R2 score in each case.\n",
    "    regdict[k] = knnreg # Store the regressors in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot it all\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(dfcars.wt, dfcars.mpg, 'o', label=\"data\")\n",
    "\n",
    "xgrid = np.linspace(np.min(dfcars.wt), np.max(dfcars.wt), 100)\n",
    "for k in [1, 2, 6, 10, 15]:\n",
    "    predictions = regdict[k].predict(xgrid.reshape(100,1))\n",
    "    if k in [1, 6, 15]:\n",
    "        ax.plot(xgrid, predictions, label=\"{}-NN\".format(k))\n",
    "    \n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we change the distance fucntion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regdict = {}\n",
    "# Do a bunch of KNN regressions\n",
    "for k in [1, 2, 4, 6, 8, 10, 15]:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors=k,p=3)\n",
    "    knnreg.fit(x_train, y_train)\n",
    "    print(knnreg.score(x_train, y_train)) # print the R2 score in each case.\n",
    "    regdict[k] = knnreg # Store the regressors in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot it all\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(dfcars.wt, dfcars.mpg, 'o', label=\"data\")\n",
    "\n",
    "xgrid = np.linspace(np.min(dfcars.wt), np.max(dfcars.wt), 100)\n",
    "for k in [1, 2, 6, 10, 15]:\n",
    "    predictions = regdict[k].predict(xgrid.reshape(100,1))\n",
    "    if k in [1, 6, 15]:\n",
    "        ax.plot(xgrid, predictions, label=\"{}-NN\".format(k))\n",
    "    \n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try a weighted knn regression, where the neighbors are weighted according to their distance, with the closest training points to the prediction points having more weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regdict = {}\n",
    "# Do a bunch of KNN regressions\n",
    "for k in [1, 2, 4, 6, 8, 10, 15]:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors=k, weights='distance')\n",
    "    knnreg.fit(x_train, y_train)\n",
    "    print(knnreg.score(x_train, y_train)) # print the R2 score in each case.\n",
    "    regdict[k] = knnreg # Store the regressors in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot it all\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(dfcars.wt, dfcars.mpg, 'o', label=\"data\")\n",
    "\n",
    "xgrid = np.linspace(np.min(dfcars.wt), np.max(dfcars.wt), 100)\n",
    "for k in [1, 2, 6, 10, 15]:\n",
    "    predictions = regdict[k].predict(xgrid.reshape(100,1))\n",
    "    if k in [1, 6, 15]:\n",
    "        ax.plot(xgrid, predictions, label=\"{}-NN\".format(k))\n",
    "    \n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that in the case of sparse data, the distance weighting schme tends to behave like the $k=1$ case, since the prediction depends strongly on the nearest training point in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Now do knn regression for the case of two or more predictors. For each case, calculate the R2 score. Use different values of $k$, just like above. Try different distance and weigthing strategies. Ellaborate about your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
